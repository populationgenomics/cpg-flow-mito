"""
Create Hail Batch jobs to run Picard tools (marking duplicates, QC).
"""

import hailtop.batch as hb
from cpg_flow import resources, utils
from cpg_utils import Path, config, hail_batch
from hailtop.batch.job import BashJob

from cpg_flow_mito.utils import get_mito_references


def markdup(
    sorted_bam: hb.Resource,
    job_attrs: dict,
    output_path: Path,
    out_markdup_metrics_path: str,
) -> BashJob:
    """
    Make job that runs Picard MarkDuplicates and converts the result to CRAM.
    """
    batch_instance = hail_batch.get_batch()
    job = batch_instance.new_bash_job(
        f'MarkDuplicates {"mito" if "mito" in str(output_path) else ""}',
        attributes=job_attrs | {'tool': 'picard_MarkDuplicates'},
    )

    job.image(config.config_retrieve(['images', 'picard']))

    # check for a memory override for impossible sequencing groups
    # if RAM is overridden, update the memory resource setting
    # check for a storage override for unreasonably large sequencing groups
    resource = resources.HIGHMEM.request_resources(
        ncpu=4,
        mem_gb=config.config_retrieve(['workflow', 'picard_mem_gb'], None),
        storage_gb=config.config_retrieve(['workflow', 'picard_storage_gb']),
    )

    resource.set_to_job(job)

    job.declare_resource_group(
        output_cram={
            'cram': '{root}.cram',
            'cram.crai': '{root}.cram.crai',
        },
    )

    fasta_reference = hail_batch.fasta_res_group(batch_instance)

    cmd = f"""
    picard {resource.java_mem_options()} MarkDuplicates \\
    I={sorted_bam} O={job.temp_bam} M={job.markdup_metrics} \\
    TMP_DIR=$(dirname {job.output_cram.cram})/picard-tmp \\
    ASSUME_SORT_ORDER=coordinate
    echo "MarkDuplicates finished successfully"

    rm {sorted_bam}

    samtools view --write-index -@{resource.get_nthreads() - 1} \\
    -T {fasta_reference.base} \\
    -O cram \\
    -o {job.output_cram.cram} \\
    {job.temp_bam}
    echo "samtools view finished successfully"
    """
    job.command(hail_batch.command(cmd, monitor_space=True))

    batch_instance.write_output(job.output_cram, str(output_path.with_suffix('')))
    batch_instance.write_output(
        job.markdup_metrics,
        out_markdup_metrics_path,
    )

    return job


def get_intervals(
    b: hb.Batch,
    scatter_count: int,
    job_attrs: dict[str, str],
    output_prefix: Path,
) -> tuple[BashJob | None, list[hb.Resource]]:
    """
    Add a job that splits genome/exome intervals into sub-intervals to be used to
    parallelize variant calling.

    @param b: Hail Batch object,
    @param scatter_count: number of target sub-intervals,
    @param job_attrs: attributes for Hail Batch job,
    @param output_prefix: path optionally to save split subintervals.

    The job calls picard IntervalListTools to scatter the input interval list
    into scatter_count sub-interval lists, inspired by this WARP task :
    https://github.com/broadinstitute/warp/blob/bc90b0db0138747685b459c83ce52c8576ce03cd/tasks/broad/Utilities.wdl

    Note that we use the mode INTERVAL_SUBDIVISION instead of
    BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW. Modes other than
    INTERVAL_SUBDIVISION produce an unpredictable number of intervals. WDL can
    handle that, but Hail Batch is not dynamic and expects a certain number
    of output files.
    """
    sequencing_type = config.config_retrieve(['workflow', 'sequencing_type'])
    source_intervals_path = config.config_retrieve(['references', f'{sequencing_type}_calling_interval_lists'])
    exclude_intervals_path = config.config_retrieve(['references', 'hg38_telomeres_and_centromeres_intervals_list'])

    if scatter_count == 1:
        # Special case when we don't need to split
        return None, [b.read_input(source_intervals_path)]

    if output_prefix:
        interval_lists_exist = all(
            utils.exists(output_prefix / f'{idx}.interval_list') for idx in range(1, scatter_count + 1)
        )
        if interval_lists_exist:
            return None, [b.read_input(str(output_prefix / f'{idx + 1}.interval_list')) for idx in range(scatter_count)]

    job = b.new_bash_job(
        f'Make {scatter_count} intervals for {sequencing_type}',
        attributes=(job_attrs or {}) | {'tool': 'picard IntervalListTools'},
    )
    job.image(config.config_retrieve(['images', 'picard']))
    resources.STANDARD.set_resources(j=job, storage_gb=16, mem_gb=2)

    break_bands_at_multiples_of = {
        'genome': 100000,
        'exome': 0,
    }.get(sequencing_type, 0)

    # If there are intervals to exclude, subtract them from the source intervals
    extra_cmd = f'-ACTION SUBTRACT -SI {b.read_input(str(exclude_intervals_path))}' if exclude_intervals_path else ''

    cmd = f"""
    set -o pipefail
    set -ex

    mkdir $BATCH_TMPDIR/out

    picard -Xms1000m -Xmx1500m \
    IntervalListTools \
    -SCATTER_COUNT {scatter_count} \
    -SUBDIVISION_MODE INTERVAL_SUBDIVISION \
    -UNIQUE true \
    -SORT true \
    -BREAK_BANDS_AT_MULTIPLES_OF {break_bands_at_multiples_of} \
    -I {b.read_input(source_intervals_path)} {extra_cmd} \
    -OUTPUT $BATCH_TMPDIR/out
    """

    for idx in range(scatter_count):
        name = f'temp_{str(idx + 1).zfill(4)}_of_{scatter_count}'
        cmd += f"""
        ln $BATCH_TMPDIR/out/{name}/scattered.interval_list {job[f'{idx + 1}.interval_list']}
        """

    job.command(cmd)

    if output_prefix:
        for idx in range(scatter_count):
            b.write_output(
                job[f'{idx + 1}.interval_list'],
                str(output_prefix / f'{idx + 1}.interval_list'),
            )

    intervals = [job[f'{idx + 1}.interval_list'] for idx in range(scatter_count)]

    return job, intervals


def collect_metrics(
    cram_path: str,
    outputs: dict[str, Path],
    job_attrs: dict,
) -> BashJob:
    """
    Run picard CollectMultipleMetrics metrics for sample QC.
    Based on https://github.com/broadinstitute/warp/blob/master/tasks/broad/Qc.wdl#L141
    """
    batch_instance = hail_batch.get_batch()

    job = batch_instance.new_job(
        'Picard CollectMultipleMetrics',
        attributes=job_attrs | {'tool': 'picard_CollectMultipleMetrics'},
    )

    job.image(config.config_retrieve(['images', 'picard']))
    # estimate CRAM size

    res = resources.STANDARD.request_resources(ncpu=2)

    sequencing_type = config.config_retrieve(['workflow', 'sequencing_type'])

    res.attach_disk_storage_gb = config.config_retrieve(['workflow', f'{sequencing_type}_cram_gb'])

    res.set_to_job(job)

    reference = hail_batch.fasta_res_group(batch_instance)

    # declare a resource group to catch all the outputs
    job.declare_resource_group(
        output_rg={
            'summary': '{root}.alignment_summary_metrics',
            'base_dist': '{root}.base_distribution_by_cycle_metrics',
            'insert_size': '{root}.insert_size_metrics',
            'qual_by_cycle': '{root}.quality_by_cycle_metrics',
            'yield': '{root}.quality_yield_metrics',
        },
    )

    # read in the CRAM and index
    cram_localised = batch_instance.read_input_group(
        cram=cram_path,
        crai=f'{cram_path}.crai',
    ).cram

    job.command(
        f"""\
    picard {res.java_mem_options()} \\
      CollectMultipleMetrics \\
      -I {cram_localised} \\
      -R {reference.base} \\
      -O $BATCH_TMPDIR/prefix \\
      -AS True \\
      --VALIDATION_STRINGENCY SILENT \\
      -PROGRAM null \\
      -PROGRAM CollectAlignmentSummaryMetrics \\
      -PROGRAM CollectInsertSizeMetrics \\
      -PROGRAM MeanQualityByCycle \\
      -PROGRAM CollectBaseDistributionByCycle \\
      -PROGRAM CollectQualityYieldMetrics \\
      -LEVEL null \\
      -LEVEL SAMPLE

    cp $BATCH_TMPDIR/prefix.alignment_summary_metrics {job.summary}
    cp $BATCH_TMPDIR/prefix.base_distribution_by_cycle_metrics {job.base_dist}
    cp $BATCH_TMPDIR/prefix.insert_size_metrics {job.insert_size}
    cp $BATCH_TMPDIR/prefix.quality_by_cycle_metrics {job.qual_by_cycle}
    cp $BATCH_TMPDIR/prefix.quality_yield_metrics {job.yield_metrics}
    """,
    )

    batch_instance.write_output(job.summary, outputs['summary'])
    batch_instance.write_output(job.base_dist, outputs['base_dist'])
    batch_instance.write_output(job.insert_size, outputs['insert_size'])
    batch_instance.write_output(job.qual_by_cycle, outputs['qual_by_cycle'])
    batch_instance.write_output(job.yield_metrics, outputs['yield'])

    return job


def hs_metrics(
    cram_path: str,
    job_attrs: dict,
    output: Path,
) -> BashJob:
    """
    Run picard CollectHsMetrics metrics.
    This method is used for exome sequencing only
    Based on https://github.com/broadinstitute/warp/blob/master/tasks/broad/Qc.wdl#L528
    """

    batch_instance = hail_batch.get_batch()

    job = batch_instance.new_bash_job(
        'Picard CollectHsMetrics', attributes=job_attrs | {'tool': 'picard_CollectHsMetrics'}
    )
    job.image(config.config_retrieve(['images', 'picard']))
    res = resources.STANDARD.request_resources(ncpu=2)
    res.attach_disk_storage_gb = config.config_retrieve(['workflow', 'exome_cram_gb'])
    res.set_to_job(job)
    reference = hail_batch.fasta_res_group(batch_instance)

    interval_file = batch_instance.read_input(config.config_retrieve(['references', 'exome_evaluation_interval_lists']))

    # read in the CRAM and index
    cram_localised = batch_instance.read_input_group(
        cram=cram_path,
        crai=f'{cram_path}.crai',
    ).cram

    job.command(
        f"""\
    # Picard is strict about the interval-list file header - contigs md5s, etc. - and
    # if md5s do not match the ref.dict file, picard would crash. So fixing the header
    # by converting the interval-list to bed (i.e. effectively dropping the header)
    # and back to interval-list (effectively re-adding the header from input ref-dict).
    # VALIDATION_STRINGENCY=SILENT does not help.
    picard IntervalListToBed \\
        -I {interval_file} \\
        -O $BATCH_TMPDIR/intervals.bed

    picard BedToIntervalList \\
        -I $BATCH_TMPDIR/intervals.bed \\
        -O $BATCH_TMPDIR/intervals.interval_list \\
        -SD {reference.dict}

    picard {res.java_mem_options()} \\
      CollectHsMetrics \\
      -I {cram_localised} \\
      -R {reference.base} \\
      --VALIDATION_STRINGENCY SILENT \\
      -TI $BATCH_TMPDIR/intervals.interval_list \\
      -BI $BATCH_TMPDIR/intervals.interval_list \\
      -LEVEL null \\
      -LEVEL SAMPLE \\
      -LEVEL LIBRARY \\
      -O {job.out_hs_metrics}
    """,
    )
    batch_instance.write_output(job.out_hs_metrics, output)
    return job


def wgs_metrics(
    cram_path: str,
    output: Path,
    job_attrs: dict,
) -> BashJob:
    """
    Run picard CollectWgsMetrics metrics.
    This method is used for genome sequencing only
    Based on https://github.com/broadinstitute/warp/blob/e1ac6718efd7475ca373b7988f81e54efab608b4/tasks/broad/Qc.wdl#L444
    """

    batch_instance = hail_batch.get_batch()

    job = batch_instance.new_bash_job(
        'Picard CollectWgsMetrics',
        attributes=job_attrs | {'tool': 'picard_CollectWgsMetrics'},
    )

    job.image(config.config_retrieve(['images', 'picard']))
    res = resources.STANDARD.request_resources(ncpu=2)
    res.attach_disk_storage_gb = config.config_retrieve(['workflow', 'genome_cram_gb'])
    res.set_to_job(job)

    reference = hail_batch.fasta_res_group(batch_instance)
    interval_file = batch_instance.read_input(
        config.config_retrieve(
            ['references', 'genome_evaluation_interval_lists'],
        ),
    )

    # read in the CRAM and index
    cram_localised = batch_instance.read_input_group(
        cram=cram_path,
        crai=f'{cram_path}.crai',
    ).cram

    job.command(
        f"""\
    picard {res.java_mem_options()} \\
      CollectWgsMetrics \\
      -I {cram_localised} \\
      -O {job.out_csv} \\
      --VALIDATION_STRINGENCY SILENT \\
      -R {reference.base} \\
      --INTERVALS {interval_file} \\
      --USE_FAST_ALGORITHM true \\
      --READ_LENGTH 250
    """,
    )
    batch_instance.write_output(job.out_csv, output)
    return job


def vcf_qc(
    gvcf: str,
    job_attrs: dict,
    output_prefix: str,
) -> BashJob:
    """Run Picard CollectVariantCallingMetrics."""

    batch_instance = hail_batch.get_batch()
    job = batch_instance.new_bash_job(
        'CollectVariantCallingMetrics', attributes=job_attrs | {'tool': 'picard CollectVariantCallingMetrics'}
    )
    job.image(config.config_retrieve(['images', 'picard']))

    res = resources.STANDARD.set_resources(j=job, storage_gb=20, mem_gb=3)

    dbsnp_vcf = config.config_retrieve(['references', 'dbsnp_vcf'])
    dbsnp_vcf_localised = batch_instance.read_input_group(
        base=dbsnp_vcf,
        index=f'{dbsnp_vcf}.idx',
    ).base
    reference = hail_batch.fasta_res_group(batch_instance)

    sequencing_type = config.config_retrieve(['workflow', 'sequencing_type'])
    intervals_file = batch_instance.read_input(
        config.config_retrieve(
            [
                'references',
                f'{sequencing_type}_evaluation_interval_lists',
            ],
        ),
    )

    gvcf_localised = batch_instance.read_input_group(
        **{
            'g.vcf.gz': gvcf,
            'g.vcf.gz.tbi': f'{gvcf}.tbi',
        },
    )['g.vcf.gz']

    # establish a resource group for the two output files
    job.declare_resource_group(
        outputs={
            'variant_calling_summary_metrics': '{root}.variant_calling_summary_metrics',
            'variant_calling_detail_metrics': '{root}.variant_calling_detail_metrics',
        },
    )

    job.command(
        f"""\
    picard {res.java_mem_options()} \
    CollectVariantCallingMetrics \
    -I {gvcf_localised} \
    -O {job.outputs} \
    --DBSNP {dbsnp_vcf_localised} \
    -SD {reference['dict']} \
    -TI {intervals_file} \
    --GVCF_INPUT true
    """,
    )

    batch_instance.write_output(job.outputs, output_prefix)

    return job
